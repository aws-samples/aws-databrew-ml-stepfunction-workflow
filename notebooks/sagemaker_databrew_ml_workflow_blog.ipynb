{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Model training workflow with AWS Glue Databrew reciepe and AWS Step Functions.\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create Resources](#Create-Resources)\n",
    "1. [Build a Machine Learning Workflow](#Build-a-Machine-Learning-Workflow)\n",
    "1. [Run the Workflow](#Run-the-Workflow)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As with growing data and large scale adoption of machine learning solutions, cleansing and visualizing data for model training has become key task in the ML workflow. As data scientist and engineers look for more insights from data they also want to reduce time to derive new insights and look for data profiling and transformation capabilities from visual preparation tool.\n",
    "As part of this blog post, we walk through a solution where we create ML workflow using AWS step functions within Sagemaker notebook and use DataBrew for visual data preparation step and run DataBrew recipe jobs as part of ML workflow.\n",
    "\n",
    "This notebook is part SageMaker model training with DataBrew recipe job blog post.\n",
    "\n",
    "## Use case overview\n",
    "\n",
    "For our use case, we use direct marketing campaigns public dataset. The marketing campaigns were based on phone calls. This dataset has few biographic and economic status about campaign contacts and their final decision to access product.\n",
    "The classification goal is to predict if the client will subscribe a term deposit (variable y).\n",
    "The dataset we use is publicly available and it is attributed by S. Moro, P. Cortez and P. Rita to the University Of California Irvine Repository Of Machine Learning Datasets.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "For our use case, this campaign CSV file is maintained by your organization’s Marketing team, which uploads CSV file to Amazon Simple Storage Service (Amazon S3). We then create series of data preparation steps using AWS DataBrew and create product subscription model using ML workflow using Sagemaker & AWS step functions SDK.\n",
    "\n",
    "* [AWS Glue Databrew](https://aws.amazon.com/glue/features/databrew/)\n",
    "* [AWS Glue Databrew Deverloper Guide](https://docs.aws.amazon.com/databrew/latest/dg/what-is.html)\n",
    "* [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "* [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n",
    "* [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Workflow \n",
    "\n",
    "![title](img/step_function_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBrew Plugin Setup (Optional)\n",
    "\n",
    "First, we'll install AWS Glue databrew plugin with Sagemaker and load all the required modules, adding this plugin will help you to work with familiar AWS Glue Databrew console within Sagemaker. \n",
    "Then we'll create fine-grained IAM roles for the AWS Glue databrew,Lambda and Step Functions resources that we will create. The IAM roles grant the services permissions within your AWS environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue Databrew Plugin Installation\n",
    "\n",
    "Make sure to add the correct permissions to the role your SageMaker Studio runs with. A good place to start is by:\n",
    "\n",
    "1. Adding the *AwsGlueDataBrewFullAccessPolicy* managed policy to your SageMaker execution role, and;\n",
    "\n",
    "2. Creating *AwsGlueDataBrewSpecificS3BucketPolicy* customer managed policy with s3:GetObject and s3:PutObject permissions for the bucket/s in which you would like to operate and adding this policy to your SageMaker execution role. Refer to IAM policy for S3 documentation for more details.\n",
    "\n",
    "Detailed steps for plugin installation is documented in below page, please use sagemaker system terminal and follow below steps.\n",
    "\n",
    "* [SageMaker Plugin installation](https://github.com/aws/aws-glue-databrew-jupyter-extension/blob/main/SageMaker-Installation-Instructions.md#installing-the-plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/sagemaker_databrew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Databrew receipe steps\n",
    "\n",
    "After creating new DataBrew project and importing dataset, please proceed with below transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data mapping\n",
    "\n",
    "Ordinal categorical values are ordered or hierarchical like Education level or economic status. These can be labeled as 1, 2 and 3 numerical format which represent lowest to highest ordering. DataBrew has an easier way to handle such variables by using Categorical mapping. It can be accessed from the toolbar under Mapping.\n",
    "Categorical mapping also used for custom one to one mapping like “Yes” / “No” to 1/0 values. Here is an example mapping for the output variable y which gets converted to numeric values 1/0 using categorical mapping function.\n",
    "\n",
    "![title](img/databrew_receipe_cat_mapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "For all non-ordinal categorical values like marital status or education, One-hot encoding is the most common way to convert them to numerical format. It can be accessed from column actions next to the column name or from the DataBrew project toolbar under Encoding. \n",
    "\n",
    "![title](img/databrew_receipe_one_hot_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning\n",
    "\n",
    "Binning is a data pre-processing technique used to reduce the effects of minor observation errors and the binning transformation allows you to group numbers of more or less continuous values into a smaller number of \"bins\". For example, in the current data set we have ages of people which we can group into a smaller number of age intervals.\n",
    "\n",
    "You can access the binning from scale menu on the toolbar.\n",
    "\n",
    "\n",
    "![title](img/databrew_receipe_binning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "\n",
    "Binarization is the process of dividing data into two groups and assigning one out of two values to all the members of the same group. You can use the Binarize transformation by defining a threshold t and assigning the value 0 to all the data points below the threshold and 1 to those above it. \n",
    "In our marketing dataset the duration column specifies the last contact duration, in seconds (numeric). This metric is highly useful to predict outcome as usually the longer the conversation the customer is interested towards the offering. Adding a binarized long call metric based on call duration will be helpful in our prediction model.\n",
    "Adding 5 min (300 sec) threshold to add new metric long call.\n",
    "\n",
    "\n",
    "![title](img/databrew_receipe_binarization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Flag column\n",
    "\n",
    "Another useful recipe step is to add a column with transformation applied on a column, we will add a flag column for those who has house but no loan to give better weightage.\n",
    "\n",
    "![title](img/databrew_receipe_flag_column.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unused columns\n",
    "\n",
    "Finally drop unused columns before writing final output file for model creation. In the next step we will use Sagemaker inbuilt model XGBoost algorithm for model training which require the output variable to be the first column in your dataset.\n",
    "\n",
    "![title](img/databrew_receipe_drop_columns_and_move_to_first.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving above recipe please create new DataBrew recipe job and use it in ETL step below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Permission Setup\n",
    "\n",
    "Before beginning this tutorial, make sure you have the required permissions to create the resources required as part of the solution.\n",
    "You have to setup below roles / permission to implement this ML workflow.\n",
    "\n",
    "Add following permission policy to your Sagemaker Studio execution role.\n",
    "1.\tAWSStepFunctionsFullAccess\n",
    "2.\tAWS Lambda function (Write for Access level)\n",
    "3.\tAWSGlueDataBrewFullAccess\n",
    "\n",
    "Create new role for Step function execution with related permission for orchestrating DataBrew Jobs, Sagemaker training & Lambda function. \t\n",
    "1.\tAmazonSageMaker-StepFunctionsWorkflowExecutionRole\n",
    "2.\tAdd Iam:PassRole permission to Sagemaker service in policy definition.\n",
    "\n",
    "AWS Glue DataBrew role to access data stored in S3 and create DataBrew recipe jobs.\n",
    "\n",
    "AWS Lambda role to query Sagemaker training status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier for you to get started, we created an AWS CloudFormation template that helps configure Sagemaker notebook role with the required policies for Step function Orchestration and new role for DataBrew job creation and AWS Lambda execution. The source code for the CloudFormation template are available in the GitHub repo.\n",
    "\n",
    "\n",
    "You have to pass below parameters to the template to pre-configure the roles to allow fine grained resource access.\n",
    "\n",
    "SageMakerExecutionRole  \t- Copy the role name from Sagemaker studio\n",
    "\n",
    "DatabrewGlueJobName\t        - Preferred name for your DataBrew recipe job\n",
    "\n",
    "S3Bucket\t\t\t        - Source data S3 bucket name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify latest version of stepfunctions.\n",
    "import sys\n",
    "\n",
    "# verify step function version\n",
    "!pip show stepfunctions\n",
    "\n",
    "# clone the repo and install SDK version > 2.2.0 required for databrew integration \n",
    "# https://github.com/aws/aws-step-functions-data-science-sdk-python/pull/151\n",
    "!git clone https://github.com/aws/aws-step-functions-data-science-sdk-python.git /tmp/aws-step-functions-data-science-sdk-python\n",
    "!pip install /tmp/aws-step-functions-data-science-sdk-python/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "id = uuid.uuid4().hex\n",
    "\n",
    "\n",
    "#Create a unique name for the AWS Glue DataBrew recipe & Glue etl job to be created. If you change the \n",
    "#default name, you may need to change the Step Functions execution role.\n",
    "recipe_job_name = 'job-marketing-research-recipe'\n",
    "etl_job_name = 'job-marketing-research-etl-{}'.format(id)\n",
    "\n",
    "#Create a unique name for the AWS Lambda function to be created. If you change\n",
    "#the default name, you may need to change the Step Functions execution role.\n",
    "function_name = 'sage_training_query_status-{}'.format(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Execution Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the AmazonSageMaker-StepFunctionsWorkflowExecutionRole ARN (please refer permission setup section)\n",
    "workflow_execution_role = ''\n",
    "\n",
    "# SageMaker Execution Role\n",
    "# You can use sagemaker.get_execution_role() if running inside sagemaker's notebook instance\n",
    "sagemaker_execution_role = sagemaker.get_execution_role() #Replace with ARN if not in an AWS SageMaker notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the query_training_status_role role ARN (refer prerequisites section)\n",
    "lambda_role = ''\n",
    "\n",
    "# paste the glue etl role this will help glue job to write to S3.\n",
    "glue_role = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "print(bucket)\n",
    "project_name = 'databrew_blog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source DataSet Location\n",
    "\n",
    "Copy the train dataset to S3 location for DataBrew transformation and to train the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = S3Uploader.upload(local_path='./data/bank-additional.csv',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                               sagemaker_session=session)\n",
    "recipe_prefix = 'recipe'\n",
    "train_prefix = 'train'\n",
    "val_prefix = 'validation'\n",
    "\n",
    "recipe_data = 's3://{}/{}/{}/'.format(bucket, project_name, recipe_prefix)\n",
    "train_data = 's3://{}/{}/{}/'.format(bucket, project_name, train_prefix)\n",
    "validation_data = 's3://{}/{}/{}/'.format(bucket, project_name, val_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Glue Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_script_location = S3Uploader.upload(local_path='./code/glue_etl.py',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                               sagemaker_session=session)\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_job(\n",
    "    Name=etl_job_name,\n",
    "    Description='PySpark job to split data in to training and validation data sets and remove header',\n",
    "    Role=glue_role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "    GlueVersion='2.0',\n",
    "    WorkerType='Standard',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the AWS Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_name = 'lambda_training_job_status.zip'\n",
    "lambda_source_code = './code/lambda_training_job_status.py'\n",
    "\n",
    "zf = zipfile.ZipFile(zip_name, mode='w')\n",
    "zf.write(lambda_source_code, arcname=lambda_source_code.split('/')[-1])\n",
    "zf.close()\n",
    "\n",
    "\n",
    "S3Uploader.upload(local_path=zip_name, \n",
    "                  desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                  sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "response = lambda_client.create_function(\n",
    "    FunctionName=function_name,\n",
    "    Runtime='python3.7',\n",
    "    Role=lambda_role,\n",
    "    Handler='lambda_training_job_status.lambda_handler',\n",
    "    Code={\n",
    "        'S3Bucket': bucket,\n",
    "        'S3Key': '{}/{}'.format(project_name, zip_name)\n",
    "    },\n",
    "    Description='Queries a SageMaker training job and return the results.',\n",
    "    Timeout=15,\n",
    "    MemorySize=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the AWS SageMaker Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost', region, '1.2-1')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    sagemaker_execution_role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, project_name))\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric='error',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Build a Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a state machine workflow to create a model training pipeline. The AWS Stepfunctions Data Science  SDK provides several AWS SageMaker workflow steps that you can use to construct an ML pipeline. In this tutorial you will create the following steps:\n",
    "\n",
    "* [**ETLStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/services.html) - Starts an AWS Glue Databrew job to extract the latest data from our source database and prepare our data.\n",
    "* [**TrainingStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep) - Creates the training step and passes the defined estimator.\n",
    "* [**ModelStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) - Creates a model in SageMaker using the artifacts created during the TrainingStep.\n",
    "* [**LambdaStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) - Creates the task state step within our workflow that calls a Lambda function.\n",
    "* [**ChoiceStateStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Choice) - Creates the choice state step within our workflow.\n",
    "* [**EndpointConfigStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) - Creates the endpoint config step to define the new configuration for our endpoint.\n",
    "* [**EndpointStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointStep) - Creates the endpoint step to update our model endpoint.\n",
    "* [**FailStateStep**](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) - Creates fail state step within our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker expects unique names for each job, model and endpoint. \n",
    "# If these names are not unique the execution will fail.\n",
    "execution_input = ExecutionInput(schema={\n",
    "    'TrainingJobName': str,\n",
    "    'DatabrewJobName': str,\n",
    "    'GlueETLJobName': str,\n",
    "    'ModelName': str,\n",
    "    'EndpointName': str,\n",
    "    'LambdaFunctionName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a step with AWS GlueDataBrew recipe Job\n",
    "In the following cell, we create a DataBrew step that runs an AWS Glue DataBrew recipe job. The Glue job extracts the latest data from our source location, removes unnecessary columns, and perform few data cleansing operations. AWS Glue DataBrew is performing this extraction, transformation, and load (ETL) in a serverless fashion, so there are no compute resources to configure and manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_step = steps.GlueDataBrewStartJobRunStep(\n",
    "    'Extract, Transform, Load',\n",
    "    parameters={\"Name\": execution_input['DatabrewJobName']}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ETL step with AWS Glue Job\n",
    "In the following cell, we create a Glue step thats runs an AWS Glue job. The Glue job splits the data in to training and validation sets, and saves the data to CSV format in S3. Glue is performing this extraction, transformation, and load (ETL) in a serverless fashion, so there are no compute resources to configure and manage. See the [GlueStartJobRunStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.GlueStartJobRunStep) Compute step in the AWS Step Functions Data Science SDK documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_step = steps.GlueStartJobRunStep('Split Train & Test DataSet',\n",
    "    parameters={\"JobName\": execution_input['GlueETLJobName'],\n",
    "                \"Arguments\":{\n",
    "                    '--S3_SOURCE': recipe_data,\n",
    "                    '--S3_DEST': 's3a://{}/{}/'.format(bucket, project_name),\n",
    "                    '--TRAIN_KEY': train_prefix + '/',\n",
    "                    '--VAL_KEY': val_prefix +'/'}\n",
    "               }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker Training Step \n",
    "\n",
    "In the following cell, we create the training step and pass the estimator we defined above. See  [TrainingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step = steps.TrainingStep(\n",
    "    'Model Training', \n",
    "    estimator=xgb,\n",
    "    data={\n",
    "        'train': TrainingInput(train_data, content_type='text/csv'),\n",
    "        'validation': TrainingInput(validation_data, content_type='text/csv')\n",
    "    },\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Model Step \n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Lambda Step\n",
    "In the following cell, we define a lambda step that will invoke the previously created lambda function as part of our Step Function workflow. See [LambdaStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_step = steps.compute.LambdaStep(\n",
    "    'Query Training Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaFunctionName'],\n",
    "        'Payload':{\n",
    "            \"TrainingJobName.$\": '$.TrainingJobName'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Choice State Step \n",
    "In the following cell, we create a choice step in order to build a dynamic workflow. This choice step branches based off of the results of our SageMaker training step: did the training job fail or should the model be saved and the endpoint be updated? We will add specific rules to this choice step later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_step = steps.states.Choice(\n",
    "    'Accuracy > 90%'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint Configuration Step\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Model Endpoint Step\n",
    "In the following cell, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = steps.EndpointStep(\n",
    "    'Update Model Endpoint',\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Fail State Step\n",
    "In addition, we create a Fail step which proceeds from our choice state if the validation accuracy of our model is lower than the threshold we define. See [FailStateStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_step = steps.states.Fail(\n",
    "    'Model Accuracy Too Low',\n",
    "    comment='Validation accuracy lower than threshold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Rules to Choice State\n",
    "In the cells below, we add a threshold rule to our choice state. Therefore, if the validation accuracy of our model is below 0.90, we move to the Fail State. If the validation accuracy of our model is above 0.90, we move to the save model step with proceeding endpoint update. See [here](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst) for more information on how XGBoost calculates classification error.\n",
    "\n",
    "For binary classification problems the XGBoost algorithm defines the model error as: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{incorret\\:predictions}{total\\:number\\:of\\:predictions}\n",
    "\\end{equation*}\n",
    "\n",
    "To achieve an accuracy of 90%, we need error <.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_rule = steps.choice_rule.ChoiceRule.NumericLessThan(variable=lambda_step.output()['Payload']['trainingMetrics'][0]['Value'], value=.1)\n",
    "\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=endpoint_config_step)\n",
    "check_accuracy_step.default_choice(next_step=fail_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link all the Steps Together\n",
    "Finally, create your workflow definition by chaining all of the steps together that we've created. See [Chain](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.states.Chain) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step.next(endpoint_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([\n",
    "    recipe_step,\n",
    "    etl_step,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    lambda_step,\n",
    "    check_accuracy_step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Workflow\n",
    "Create your workflow using the workflow definition above, and render the graph with [render_graph](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.render_graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Workflow(\n",
    "    name='MarketingCampaignInference_{}'.format(id),\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")\n",
    "\n",
    "# render workflow graph\n",
    "workflow.render_graph()\n",
    "\n",
    "# create workflow\n",
    "workflow.create()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the workflow\n",
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'TrainingJobName': 'regression-{}'.format(id), # Each Sagemaker Job requires a unique name,\n",
    "        'DatabrewJobName': recipe_job_name,\n",
    "        'GlueETLJobName': etl_job_name,\n",
    "        'ModelName': 'MarketingCampaignPrediction-{}'.format(id), # Each Model requires a unique name,\n",
    "        'EndpointName': 'MarketingCampaign', # Each Endpoint requires a unique name\n",
    "        'LambdaFunctionName': function_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step function output\n",
    "\n",
    "![title](img/step_function_output.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "When you are done, make sure to clean up your AWS account by deleting resources you won't be reusing."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
